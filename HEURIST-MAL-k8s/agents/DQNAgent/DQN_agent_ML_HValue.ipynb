{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6930e24f",
   "metadata": {},
   "source": [
    "## Install Gym environments to interact with k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../../gym_k8s_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "from threading import Lock, Thread\n",
    "import datetime\n",
    "import gym_k8s_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75825c11",
   "metadata": {},
   "source": [
    "## Packages to build DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c72e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d7d75",
   "metadata": {},
   "source": [
    "## Define DQN class - with Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365223b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class DQNAgent():\n",
    "    def __init__(self, env, path, episodes, max_env_steps, win_threshold, epsilon_decay,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = env\n",
    " \n",
    "        if state_size is None: \n",
    "            self.state_size = self.env.observation_space.n \n",
    "        else: \n",
    "            self.state_size = state_size\n",
    " \n",
    "        if action_size is None: \n",
    "            self.action_size = self.env.action_space.n \n",
    "        else: \n",
    "            self.action_size = action_size\n",
    " \n",
    "        self.episodes = episodes\n",
    "        self.env._max_episode_steps = max_env_steps\n",
    "        self.win_threshold = win_threshold\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path                     #location where the model is saved to\n",
    "        self.prints = prints                 #if true, the agent will print his scores\n",
    " \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    #Build network model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(48, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "        return model\n",
    "    \n",
    "    #Generate one action\n",
    "    def generate_action(self, state, eps, hAction):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        # select action with herustic-boosted method\n",
    "        hVal = self.hValue(state, hAction)\n",
    "        maxAction = -1\n",
    "        maxQ = -1\n",
    "        for action in range(len(self.model.predict(state)[0])):\n",
    "            aggreVal = self.model.predict(state)[0][action] + hVal if action == hAction else self.model.predict(new_state)[0][action] + 0\n",
    "            if maxQ < aggreVal:\n",
    "                maxQ = aggreVal\n",
    "                maxAction = action\n",
    "        return maxAction\n",
    "    \n",
    "    #Add states into memory\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #Replay memory to train\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    #Define calculate heuristic value; input: action from heuristic policy\n",
    "    #hAction is returned from gym environment and is encoded \n",
    "    def hValue(self, state, hAction):\n",
    "        #Q-value where action is equal to heuristic action\n",
    "        heurQ = self.target_model.predict(state)[0][hAction]\n",
    "        hVal = np.amax(self.target_model.predict(state)[0])\n",
    "        return hVal - heurQ\n",
    "        \n",
    "    #Actual training process\n",
    "    def learning(self, hAction):\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.memory) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "            \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "    \n",
    "    #Save model\n",
    "    def save_model(self, name='DQN_Model'):\n",
    "        self.model.save(self.path + name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceada8bc",
   "metadata": {},
   "source": [
    "## Info about the kubernetes environment we deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e56eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep duration in minutes\n",
    "# We wait these many minutes for our actions to be enforced\n",
    "timestep_duration = 1.5\n",
    "current_app_names = [\"firewall\", \"encrypt\", \"decrypt\"]\n",
    "current_cluster_names = [\"kubernetes-master-dev\", \"kubernetes-work-dev\"]\n",
    "memory_req = '128Mi'\n",
    "cpu_req = '80m'\n",
    "sla_latency = 2.6\n",
    "sla_host = 'http://145.100.135.89:6088/'\n",
    "# latency metric\n",
    "sla_metric_name = 'latency'\n",
    "gym_env = 'gym_k8s_real:k8s-env-dqn-v0'\n",
    "#init Q_table\n",
    "total_epochs = 100\n",
    "num_of_services = 1\n",
    "steps_per_epoch = 15\n",
    "app_dict = {\"firewall\": 500, \"encrypt\": 500, \"decrypt\": 500}\n",
    "config_path = \"~/EPI_kube_scaling/EPIF-Configurations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec10342",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_names=current_app_names,\n",
    "        app_configs = ['s', 'm', 'l'], \n",
    "        cluster_names = current_cluster_names, \n",
    "        sla_latency = sla_latency,\n",
    "        sla_host = sla_host, \n",
    "        sla_latency_metric_name = sla_metric_name, \n",
    "        max_pods = 30, \n",
    "        min_pods = 1,\n",
    "        app_dict = app_dict,\n",
    "        config_path = config_path,\n",
    "        changes = [0, 1, 2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fb75",
   "metadata": {},
   "source": [
    "## Create historical states csv file if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e700dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    open('k8s_historical_states.csv', 'r').close()\n",
    "    print('File already present.')\n",
    "except IOError:\n",
    "    with open('k8s_historical_states.csv', 'w') as f:\n",
    "        #TO-DO: define states you want to collect\n",
    "#         f.write()\n",
    "    print('File not present. Created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8f74b",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839d35c",
   "metadata": {},
   "source": [
    "## Agent training\n",
    "This function trains our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    \n",
    "    #make sure env is maked\n",
    "    env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_name=current_app_name,\n",
    "        sla_throughput=sla_latency,\n",
    "        prometheus_host=sla_host,\n",
    "        prometheus_throughput_metric_name=current_throughput_metric_name\n",
    "    )\n",
    "    \n",
    "    #define the DQN agent\n",
    "    agent = DQNAgent(env, \"\", 100, 10, 0.1, 10,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False)\n",
    "    \n",
    "    for epoch in range(0, total_epochs):\n",
    "        state, _ = env.reset()\n",
    "        #TODO: get hAction from heuristic method\n",
    "        hAction = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            current_timestep = epoch * steps_per_epoch + step\n",
    "        \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else: \n",
    "                action = agent.generate_action(state, epsilon, hAction)\n",
    "                        \n",
    "                real_ob, reward, done, next_state = env.step(action)\n",
    "                    \n",
    "                #Update Q-value\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                #Try one round of training\n",
    "                agent.learning()\n",
    "                \n",
    "                #To-do: Update H-value\n",
    "            \n",
    "                #Update state\n",
    "                state = next_state\n",
    "                \n",
    "        agent.save_model()\n",
    "        print('One epoch of training finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac98b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
