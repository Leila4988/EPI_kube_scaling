{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../../gym_k8s_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "from threading import Lock, Thread\n",
    "import datetime\n",
    "import gym_k8s_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75825c11",
   "metadata": {},
   "source": [
    "## Packages to build DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c72e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d7d75",
   "metadata": {},
   "source": [
    "## Define DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0365223b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class DQNAgent():\n",
    "    def __init__(self, env, path, episodes, max_env_steps, win_threshold, epsilon_decay,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = env\n",
    " \n",
    "        if state_size is None: \n",
    "            self.state_size = self.env.observation_space.n \n",
    "        else: \n",
    "            self.state_size = state_size\n",
    " \n",
    "        if action_size is None: \n",
    "            self.action_size = self.env.action_space.n \n",
    "        else: \n",
    "            self.action_size = action_size\n",
    " \n",
    "        self.episodes = episodes\n",
    "        self.env._max_episode_steps = max_env_steps\n",
    "        self.win_threshold = win_threshold\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path                     #location where the model is saved to\n",
    "        self.prints = prints                 #if true, the agent will print his scores\n",
    " \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    #Build network model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(48, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "        return model\n",
    "    \n",
    "    #Generate one action\n",
    "    def generate_action(self, state, eps, hAction):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        # select action with herustic-boosted method\n",
    "        hVal = self.hValue(state, hAction)\n",
    "        maxAction = -1\n",
    "        maxQ = -1\n",
    "        for action in range(len(self.target_model.predict(state)[0])):\n",
    "            aggreVal = self.target_model.predict(state)[0][action] + hVal if action == hAction else self.target_model.predict(new_state)[0][action] + 0\n",
    "            if maxQ < aggreVal:\n",
    "                maxQ = aggreVal\n",
    "                maxAction = action\n",
    "        return maxAction\n",
    "    \n",
    "    #Add states into memory\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #Replay memory to train\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    #Define calculate heuristic value; input: action from heuristic policy\n",
    "    #hAction is returned from gym environment and is encoded \n",
    "    def hValue(self, state, hAction):\n",
    "        #Q-value where action is equal to heuristic action\n",
    "        heurQ = self.target_model.predict(state)[0][hAction]\n",
    "        hVal = np.amax(self.target_model.predict(state)[0])\n",
    "        return hVal - heurQ\n",
    "        \n",
    "    #Actual training process\n",
    "    def learning(self, hAction):\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.replay_buffer) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.replay_buffer, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "            \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "    \n",
    "    #Save model\n",
    "    def save_model(self, name='DQN_Model'):\n",
    "        self.model.save(self.path + name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceada8bc",
   "metadata": {},
   "source": [
    "## Info about the kubernetes environment we deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e56eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep duration in minutes\n",
    "# We wait these many minutes for our actions to be enforced\n",
    "timestep_duration = 1.5\n",
    "app_name = 'firewall'\n",
    "memory_req = '128Mi'\n",
    "cpu_req = '80m'\n",
    "sla_latency = 2.6\n",
    "sla_host = 'http://145.100.135.89:6088/'\n",
    "# latency metric\n",
    "sla_metric_name = 'latency'\n",
    "gym_env = 'gym_k8s_real:k8s-env-discrete-state-discrete-action-v2'\n",
    "#init Q_table\n",
    "q_table_file = 'Q-env-discrete-state-discrete-action-data.npy'\n",
    "q_table_init_value = 12.5\n",
    "total_epochs = 100\n",
    "num_of_services = 1\n",
    "steps_per_epoch = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fb75",
   "metadata": {},
   "source": [
    "## Create historical states csv file if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e700dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not present. Created successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    open('k8s_historical_states.csv', 'r').close()\n",
    "    print('File already present.')\n",
    "except IOError:\n",
    "    with open('k8s_historical_states.csv', 'w') as f:\n",
    "        f.write('current_app_name,timestep,state,action,next_state,reward,'\n",
    "                'done,number_of_pods,cpu_util,latency_violation,latency,hpa_threshold,info\\n')\n",
    "    print('File not present. Created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8f74b",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839d35c",
   "metadata": {},
   "source": [
    "## Agent training\n",
    "This function trains our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_service):\n",
    "    # Hyperparameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon_init = 0.97\n",
    "    epsilon_min = 0.2\n",
    "\n",
    "    current_app_name = app_name\n",
    "    current_throughput_metric_name = prometheus_throughput_metric_name\n",
    "    \n",
    "    env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_name=current_app_name,\n",
    "        sla_throughput=sla_throughput,\n",
    "        prometheus_host=prometheus_host,\n",
    "        prometheus_throughput_metric_name=current_throughput_metric_name\n",
    "    )\n",
    "\n",
    "    agent = DQNAgent(env, \"\", episodes, max_env_steps, win_threshold, epsilon_decay,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False)\n",
    "    \n",
    "    for epoch in range(0, total_epochs):\n",
    "        state, _, hAction = env.reset()\n",
    "#         state, _ = env.reset()\n",
    "#         decoded_state = list(decode(state))\n",
    "#         print('======EPOCH{}=======\\n'\n",
    "#               'Training started with cpu utilization: {}, hpa cpu threshold: {}, number of pods: {}, latency:{}'\n",
    "#              .format(epoch, decoded_state[0], decoded_state[1], decoded_state[2], decoded_state[3]))\n",
    "     \n",
    "        done = False\n",
    "        \n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            current_timestep = epoch * steps_per_epoch + step\n",
    "            q_table = np.load(q_table_file)\n",
    "            \n",
    "            # Epsilon keeps getting smaller and stops when it reaches epsilon_min\n",
    "            current_epsilon = pow(epsilon_init, current_timestep)\n",
    "            epsilon = max(current_epsilon, epsilon_min)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else: \n",
    "                action = agent.generate_action(state, epsilon)\n",
    "                \n",
    "#                 decoded_state = list(decode(state))\n",
    "#                 print('======ROUND{}=======\\n'\n",
    "#                       'app: {}, pod_cpu_util: {}, cpu_threshold: {}, number_of_pods: {}, latency: {}, latency_violation: {}, action: {}'\n",
    "#                       .format(step, current_app_name, decoded_state[0], decoded_state[1], decoded_state[2], \n",
    "#                               decoded_state[3], int(decoded_state[3] >= 5), action))\n",
    "                \n",
    "                real_ob, reward, done, next_state, hAction = env.step(action)\n",
    "#                 real_ob, reward, done, next_state = env.step(action)\n",
    "                \n",
    "                now = datetime.datetime.now() + datetime.timedelta(hours=2)\n",
    "                dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "                dt_dict = {\n",
    "                    'datetime': dt_string\n",
    "                }\n",
    "                info = dt_dict\n",
    "                \n",
    "                #To-do: change real_ob\n",
    "                (pod_cpu_util,\n",
    "                 cpu_threshold,\n",
    "                 number_of_pods,\n",
    "                 latency) = real_ob\n",
    "\n",
    "                # Latency violation becomes 1 if the SLA was violated\n",
    "                # otherwise it's 0\n",
    "                latency_violation = int(latency > sla_latency)\n",
    "                \n",
    "\n",
    "                # Save historical tuple\n",
    "                with open('k8s_historical_states_discrete.csv', 'a') as f:\n",
    "                    f.write(\n",
    "                        '{},{},{},{},'.format(current_app_name, current_timestep, state, action) +\n",
    "                        '{},{},{},{},'.format(next_state, reward, done, number_of_pods) +\n",
    "                        '{},{},{},{},{}'.format(pod_cpu_util, throughput_violation, throughput, cpu_threshold, info) +\n",
    "                        '\\n'\n",
    "                    )\n",
    "                    \n",
    "                #Update Q-value\n",
    "                agent.remember(state, action, reward, next_state, done): \n",
    "                \n",
    "                #Try one round of training\n",
    "                agent.learning()\n",
    "                \n",
    "                #To-do: Update H-value\n",
    "            \n",
    "                #Update state\n",
    "                state = next_state\n",
    "                \n",
    "        agent.save_model()\n",
    "        print('One epoch of training finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b454d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(i):\n",
    "        out = []\n",
    "        out.append(i % 7)\n",
    "        i //= 7\n",
    "        out.append(i % 5)\n",
    "        i //= 5\n",
    "        out.append(i % 5)\n",
    "        i //= 5\n",
    "        out.append(i)\n",
    "        return reversed(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f7e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_agent(num_of_services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac98b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
