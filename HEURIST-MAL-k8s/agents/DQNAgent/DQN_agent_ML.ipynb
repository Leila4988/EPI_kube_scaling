{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f0bfa3",
   "metadata": {},
   "source": [
    "## Install Gym environments to interact with k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf5dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/li/EPI-kube-scaling/HEURIST-MAL-k8s/gym_k8s_real\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (0.21.0)\n",
      "Requirement already satisfied: pint in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (0.19.2)\n",
      "Requirement already satisfied: pyyaml in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (6.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from gym-k8s-real==0.0.1) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/li/.local/lib/python3.9/site-packages (from gym->gym-k8s-real==0.0.1) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/li/.local/lib/python3.9/site-packages (from gym->gym-k8s-real==0.0.1) (2.0.0)\n",
      "Installing collected packages: gym-k8s-real\n",
      "  Attempting uninstall: gym-k8s-real\n",
      "    Found existing installation: gym-k8s-real 0.0.1\n",
      "    Uninstalling gym-k8s-real-0.0.1:\n",
      "      Successfully uninstalled gym-k8s-real-0.0.1\n",
      "  Running setup.py develop for gym-k8s-real\n",
      "Successfully installed gym-k8s-real\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ../../gym_k8s_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a486ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "from threading import Lock, Thread\n",
    "import datetime as dtime\n",
    "import gym_k8s_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75825c11",
   "metadata": {},
   "source": [
    "## Packages to build DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c72e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 03:34:49.771036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-21 03:34:49.771096: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d7d75",
   "metadata": {},
   "source": [
    "## Define DQN class - without Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0365223b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class DQNAgent():\n",
    "    def __init__(self, env, path, episodes, max_env_steps, win_threshold, epsilon_decay,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False, step=0):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = env\n",
    "        self.t = 0\n",
    "        self.C = episodes\n",
    "        \n",
    "        if state_size is None: \n",
    "            self.state_size = self.env.observation_space.n \n",
    "        else: \n",
    "            self.state_size = state_size\n",
    " \n",
    "        if action_size is None: \n",
    "            self.action_size = self.env.action_space.n \n",
    "        else: \n",
    "            self.action_size = action_size\n",
    " \n",
    "        self.episodes = episodes\n",
    "        self.step = step\n",
    "        self.env._max_episode_steps = max_env_steps\n",
    "        self.win_threshold = win_threshold\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path                     #location where the model is saved to\n",
    "        self.prints = prints                 #if true, the agent will print his scores\n",
    " \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    #Build network model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(48, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "        return model\n",
    "    \n",
    "    #Generate one action\n",
    "    def generate_action(self, state):\n",
    "        # Epsilon keeps getting smaller and stops when it reaches epsilon_min\n",
    "        current_epsilon = pow(self.epsilon, self.step)\n",
    "        eps = max(current_epsilon, self.epsilon_min)\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    #Add states into memory\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #Replay memory to train\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    #Actual training process\n",
    "    def learning(self):\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.memory) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "            \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "    \n",
    "    #Save model\n",
    "    def save_model(self, name='DQN_Model_ML'):\n",
    "        self.model.save(self.path + name)\n",
    "    \n",
    "    def load_model(self, name='DQN_Model_ML'):\n",
    "        self.model = load_model(self.path + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceada8bc",
   "metadata": {},
   "source": [
    "## Info about the kubernetes environment we deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e56eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep duration in minutes\n",
    "# We wait these many minutes for our actions to be enforced\n",
    "timestep_duration = 1\n",
    "ml_app_names = [\"firewall\", \"encrypt\"]\n",
    "ml_cluster_names = [\"cluster1-cntx\", \"cluster2-cntx\", \"cluster3-cntx\"]\n",
    "memory_req = '128Mi'\n",
    "cpu_req = '80m'\n",
    "sla_latency = 2.6\n",
    "sla_host = 'http://145.100.135.89:6088/'\n",
    "# latency metric\n",
    "sla_metric_name = 'latency'\n",
    "gym_env = 'gym_k8s_real:k8s-env-dqn-ml-v0'\n",
    "#init Q_table\n",
    "total_epochs = 100\n",
    "num_of_services = 1\n",
    "steps_per_epoch = 15\n",
    "app_dict = {\"firewall\": 500, \"encrypt\": 500}\n",
    "config_path = \"\"\n",
    "#dict describes the connection between clusters\n",
    "ml_connection = {0: [0, 1, 2], 1: [0, 1, 2], 2: [0, 1, 2]}\n",
    "ml_pod_weight = 1\n",
    "ml_latency_weight = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef420153",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_names=ml_app_names,\n",
    "        app_configs = ['s', 'm', 'l'], \n",
    "        cluster_names = ml_cluster_names, \n",
    "        sla_latency = sla_latency,\n",
    "        sla_host = sla_host, \n",
    "        sla_latency_metric_name = sla_metric_name, \n",
    "        max_pods = 20, \n",
    "        min_pods = 1,\n",
    "        app_dict = app_dict,\n",
    "        config_path = config_path,\n",
    "        changes = [0, 1, 2],\n",
    "        connection = ml_connection,\n",
    "        pod_weight = ml_pod_weight,\n",
    "        latency_weight = ml_latency_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fb75",
   "metadata": {},
   "source": [
    "## Create historical states csv file if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e700dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not present. Created successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    open('k8s_DQN_ML_historical_states.csv', 'r').close()\n",
    "    print('File already present.')\n",
    "except IOError:\n",
    "    with open('k8s_DQN_ML_historical_states.csv', 'w') as f:\n",
    "        #TO-DO: define states you want to collect\n",
    "        f.write('app_name,timestep,reward,'\n",
    "                'cpu-cluster1,cpu-cluster2,cpu-cluster3,'\n",
    "                'noPod-cluster1,noPod-cluster2,noPod-cluster3'\n",
    "                'latency,latency_violation,datetime\\n')\n",
    "        print('File not present. Created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8f74b",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839d35c",
   "metadata": {},
   "source": [
    "## Agent training\n",
    "This function trains our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    \n",
    "    #make sure env is maked\n",
    "    env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_names=ml_app_names,\n",
    "        app_configs = ['s', 'm', 'l'], \n",
    "        cluster_names = ml_cluster_names, \n",
    "        sla_latency = sla_latency,\n",
    "        sla_host = sla_host, \n",
    "        sla_latency_metric_name = sla_metric_name, \n",
    "        max_pods = 20, \n",
    "        min_pods = 1,\n",
    "        app_dict = app_dict,\n",
    "        config_path = config_path,\n",
    "        changes = [0, 1, 2],\n",
    "        connection = ml_connection,\n",
    "        pod_weight = ml_pod_weight,\n",
    "        latency_weight = ml_latency_weight\n",
    "    )\n",
    "    \n",
    "    #define the DQN agent\n",
    "    agent = DQNAgent(env, \"\", 100, 10, 0.1, 10,\n",
    "                 state_size=None, action_size=None, epsilon=1.0, epsilon_min=0.01, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False)\n",
    "    \n",
    "    for epoch in range(0, total_epochs):\n",
    "        state, _ = env.reset()\n",
    "        hAction = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            current_timestep = epoch * steps_per_epoch + step\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            else: \n",
    "                action = agent.generate_action(state)\n",
    "                        \n",
    "                real_ob, reward, done, next_state = env.step(action)\n",
    "                print(real_ob)\n",
    "#                 f.write('app_name,timestep,reward,'\n",
    "#                 'cpu-cluster1,cpu-cluster2,cpu-cluster3,'\n",
    "#                 'noPod-cluster1,noPod-cluster2,noPod-cluster3',\n",
    "#                 'latency,latency_violation,info\\n')\n",
    "\n",
    "                now = dtime.datetime.now() + dtime.timedelta(hours=2)\n",
    "                dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "                dt_dict = {\n",
    "                    'datetime': dt_string\n",
    "                }\n",
    "                datetime = dt_dict\n",
    "                \n",
    "                with open('k8s_DQN_ML_historical_states.csv', 'a') as f:\n",
    "                    apps = ['firewall', 'encrypt']\n",
    "                    for i in range(2):\n",
    "                        f.write(\n",
    "                            '{},{},{},'.format(apps[i], current_timestep, reward) +\n",
    "                            '{},{},{},'.format(real_ob[0][i][0], real_ob[0][i][1], real_ob[0][i][2]) +\n",
    "                            '{},{},{},'.format(real_ob[1][i][0], real_ob[1][i][1], real_ob[1][i][2]) +\n",
    "                            '{},{},{}'.format(real_ob[3], int(real_ob[3]>sla_latency), datetime) +\n",
    "                            '\\n'\n",
    "                        )\n",
    "                    \n",
    "                #Update Q-value\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                #Try one round of training\n",
    "                agent.learning()\n",
    "                \n",
    "                #To-do: Update H-value\n",
    "            \n",
    "                #Update state\n",
    "                state = next_state\n",
    "                \n",
    "        agent.save_model()\n",
    "        print('One epoch of training finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac98b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/li/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action-idx:78, config-idx: 1, app_idx: 0, cluster_idx: 1, action: 1, proxy_idx: 0\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:71, config-idx: 2, app_idx: 1, cluster_idx: 1, action: 0, proxy_idx: 2\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:65, config-idx: 1, app_idx: 1, cluster_idx: 1, action: 0, proxy_idx: 2\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:104, config-idx: 2, app_idx: 0, cluster_idx: 1, action: 2, proxy_idx: 1\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:25, config-idx: 1, app_idx: 1, cluster_idx: 0, action: 1, proxy_idx: 0\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:118, config-idx: 1, app_idx: 0, cluster_idx: 2, action: 0, proxy_idx: 2\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:98, config-idx: 1, app_idx: 0, cluster_idx: 1, action: 2, proxy_idx: 1\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:78, config-idx: 1, app_idx: 0, cluster_idx: 1, action: 1, proxy_idx: 0\n",
      "[[''], ['miss'], ['miss']]\n",
      "deployment.apps/proxy0 patched (no change)\n",
      "[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0, 5, 1], [5, 0, 0]], [1, 3], 30]\n",
      "action-idx:90, config-idx: 0, app_idx: 0, cluster_idx: 1, action: 2, proxy_idx: 0\n",
      "[[''], ['miss'], ['miss']]\n",
      "deployment.apps/proxy0 patched (no change)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     42\u001b[0m                 action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mgenerate_action(state)\n\u001b[0;32m---> 44\u001b[0m                 real_ob, reward, done, next_state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(real_ob)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#                 f.write('app_name,timestep,reward,'\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#                 'cpu-cluster1,cpu-cluster2,cpu-cluster3,'\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#                 'noPod-cluster1,noPod-cluster2,noPod-cluster3',\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#                 'latency,latency_violation,info\\n')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/EPI-kube-scaling/HEURIST-MAL-k8s/gym_k8s_real/gym_k8s_real/envs/k8s_env_DQN_ML.py:132\u001b[0m, in \u001b[0;36mK8sEnvDQNML.step\u001b[0;34m(self, action_idx)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_proxy():\n\u001b[1;32m    131\u001b[0m     wait_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_duration \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait timestep_duration minutes for the changes to take place\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     encoded_observation, real_observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_state_max_latency()\n\u001b[1;32m    134\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c671b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
