{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f0bfa3",
   "metadata": {},
   "source": [
    "## Install Gym environments to interact with k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf5dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/li/EPI-kube-scaling/HEURIST-MAL-k8s/gym_k8s_real\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (0.21.0)\n",
      "Requirement already satisfied: pint in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (0.19.2)\n",
      "Requirement already satisfied: pyyaml in /home/li/.local/lib/python3.9/site-packages (from gym-k8s-real==0.0.1) (6.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from gym-k8s-real==0.0.1) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/li/.local/lib/python3.9/site-packages (from gym->gym-k8s-real==0.0.1) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/li/.local/lib/python3.9/site-packages (from gym->gym-k8s-real==0.0.1) (2.0.0)\n",
      "Installing collected packages: gym-k8s-real\n",
      "  Attempting uninstall: gym-k8s-real\n",
      "    Found existing installation: gym-k8s-real 0.0.1\n",
      "    Uninstalling gym-k8s-real-0.0.1:\n",
      "      Successfully uninstalled gym-k8s-real-0.0.1\n",
      "  Running setup.py develop for gym-k8s-real\n",
      "Successfully installed gym-k8s-real\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ../../gym_k8s_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a486ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "from threading import Lock, Thread\n",
    "import datetime as dtime\n",
    "import gym_k8s_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75825c11",
   "metadata": {},
   "source": [
    "## Packages to build DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c72e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d7d75",
   "metadata": {},
   "source": [
    "## Define DQN class - without Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0365223b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class DQNAgent():\n",
    "    def __init__(self, env, path, episodes, max_env_steps, epsilon_decay,\n",
    "                 state_size=None, action_size=None, epsilon=0.97, epsilon_min=0.1, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False, step=0):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = env\n",
    "        self.t = 0\n",
    "        self.C = episodes\n",
    "        \n",
    "        if state_size is None: \n",
    "            self.state_size = self.env.observation_space.n \n",
    "        else: \n",
    "            self.state_size = state_size\n",
    " \n",
    "        if action_size is None: \n",
    "            self.action_size = self.env.action_space.n \n",
    "        else: \n",
    "            self.action_size = action_size\n",
    " \n",
    "        self.step = step\n",
    "        self.env._max_episode_steps = max_env_steps\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path                     #location where the model is saved to\n",
    "        self.prints = prints                 #if true, the agent will print his scores\n",
    " \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    #Build network model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(48, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "        return model\n",
    "    \n",
    "    #Generate one action\n",
    "    def generate_action(self, state):\n",
    "        # Epsilon keeps getting smaller and stops when it reaches epsilon_min\n",
    "        current_epsilon = pow(self.epsilon, self.step)\n",
    "        eps = max(current_epsilon, self.epsilon_min)\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    #Add states into memory\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #Replay memory to train\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    #Actual training process\n",
    "    def learning(self):\n",
    "        self.step += 1\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.memory) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "            \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "    \n",
    "    #Save model\n",
    "    def save_model(self, name='DQN_Model_ML'):\n",
    "        self.model.save(self.path + name)\n",
    "    \n",
    "    def load_model(self, name='DQN_Model_ML'):\n",
    "        self.model = load_model(self.path + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceada8bc",
   "metadata": {},
   "source": [
    "## Configuration about the kubernetes environment we deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3e56eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep duration in minutes\n",
    "# We wait these many minutes for our actions to be enforced\n",
    "timestep_duration = 1\n",
    "ml_app_names = [\"firewall\", \"encrypt\"]\n",
    "ml_cluster_names = [\"cluster1-cntx\", \"cluster2-cntx\", \"cluster3-cntx\"]\n",
    "#start: don't need to change\n",
    "memory_req = '128Mi'\n",
    "cpu_req = '80m'\n",
    "app_dict = {\"firewall\": 500, \"encrypt\": 500}\n",
    "#end\n",
    "sla_latency = 2.6\n",
    "#link: http://145.100.135.89:6088/latency\n",
    "sla_host = 'http://145.100.135.89:6088/'\n",
    "# latency metric\n",
    "sla_metric_name = 'latency'\n",
    "gym_env = 'gym_k8s_real:k8s-env-dqn-ml-v0'\n",
    "#init Q_table\n",
    "total_epochs = 10\n",
    "num_of_services = 1\n",
    "steps_per_epoch = 100\n",
    "config_path = \"\"\n",
    "#dict describes the connection between clusters\n",
    "ml_connection = {0: [0, 1, 2], 1: [0, 1, 2], 2: [0, 1, 2]}\n",
    "ml_pod_weight = 1 #resouce rewards\n",
    "ml_latency_weight = 1.5 #performance rewards\n",
    "#reward = ml_pod_weight * Rres + ml_latency_weight * Rperf\n",
    "#ml_pod_weight + ml_latency_weight = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fb75",
   "metadata": {},
   "source": [
    "## Create historical states csv file if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e700dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not present. Created successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    open('k8s_DQN_ML_historical_states.csv', 'r').close()\n",
    "    print('File already present.')\n",
    "except IOError:\n",
    "    with open('k8s_DQN_ML_historical_states.csv', 'w') as f:\n",
    "        f.write('app_name,timestep,reward,'\n",
    "                'cpu-pods,cpu-podm,cpu-podl,'\n",
    "                'noPod-cluster1,noPod-cluster2,noPod-cluster3,'\n",
    "                'latency,latency_violation,datetime\\n')\n",
    "        print('File not present. Created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8f74b",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839d35c",
   "metadata": {},
   "source": [
    "## Agent training\n",
    "This function trains our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e8fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    \n",
    "    #make sure env is maked\n",
    "    env = gym.make(\n",
    "        gym_env,\n",
    "        timestep_duration=timestep_duration,\n",
    "        app_names=ml_app_names,\n",
    "        app_configs = ['s', 'm', 'l'], \n",
    "        cluster_names = ml_cluster_names, \n",
    "        sla_latency = sla_latency,\n",
    "        sla_host = sla_host, \n",
    "        sla_latency_metric_name = sla_metric_name, \n",
    "        max_pods = 20, \n",
    "        min_pods = 1,\n",
    "        app_dict = app_dict,\n",
    "        config_path = config_path,\n",
    "        changes = [0, 1, 2],\n",
    "        connection = ml_connection,\n",
    "        pod_weight = ml_pod_weight,\n",
    "        latency_weight = ml_latency_weight\n",
    "    )\n",
    "    \n",
    "    #define the DQN agent\n",
    "    #path: the path to save or load DQN model\n",
    "    #episodes: the number of rounds between each updation of DQN model\n",
    "    #epsilon: the probability that selects random actions\n",
    "    #epsilon_min: the minimal probability that selects random actions\n",
    "    #gamma and alpha: hyperparameters to update Q-value in RL algorithm\n",
    "    #batch_size: the number of rounds or sample data for a batch of updation\n",
    "    agent = DQNAgent(env, path = \"\", episodes = 20, max_env_steps = 10, epsilon_decay = 10,\n",
    "                 state_size=None, action_size=None, epsilon=0.97, epsilon_min=0.1, \n",
    "                 gamma=1, alpha=.01, alpha_decay=.01, batch_size=16, prints=False)\n",
    "    \n",
    "    for epoch in range(0, total_epochs):\n",
    "        #each epoch represents a new training, since you need to reset the environment\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        #steps_per_epoch: how many rounds or actions you want the agent to perform in each epoch of training\n",
    "        for step in range(steps_per_epoch):\n",
    "            current_timestep = epoch * steps_per_epoch + step\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else: \n",
    "                action = agent.generate_action(state)\n",
    "                        \n",
    "                #let the environment to take one step of action\n",
    "                real_ob, reward, done, next_state = env.step(action)\n",
    "                print(\"state right now:\", real_ob)\n",
    "#                 f.write('app_name,timestep,reward,'\n",
    "#                 'cpu-cluster1,cpu-cluster2,cpu-cluster3,'\n",
    "#                 'noPod-cluster1,noPod-cluster2,noPod-cluster3',\n",
    "#                 'latency,latency_violation,info\\n')\n",
    "\n",
    "                now = dtime.datetime.now() + dtime.timedelta(hours=2)\n",
    "                dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "                dt_dict = {\n",
    "                    'datetime': dt_string\n",
    "                }\n",
    "                datetime = dt_dict\n",
    "                \n",
    "                #save historical data\n",
    "                with open('k8s_DQN_ML_historical_states.csv', 'a') as f:\n",
    "                    apps = ['firewall', 'encrypt']\n",
    "                    for i in range(2):\n",
    "                        f.write(\n",
    "                            '{},{},{},'.format(apps[i], current_timestep, reward) +\n",
    "                            '{},{},{},'.format(real_ob[0][i][0], real_ob[0][i][1], real_ob[0][i][2]) +\n",
    "                            '{},{},{},'.format(real_ob[1][i][0], real_ob[1][i][1], real_ob[1][i][2]) +\n",
    "                            '{},{},{}'.format(real_ob[3], int(real_ob[3]>sla_latency), datetime) +\n",
    "                            '\\n'\n",
    "                        )\n",
    "                    \n",
    "                #save this step in the memory of DQN model\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                #call learn method of the model; it will determine if to update the model according to some parameters\n",
    "                agent.learning()\n",
    "                \n",
    "                #move to the latest state\n",
    "                state = next_state\n",
    "                \n",
    "        #save model in the end of each epoch, make sure data will not lose\n",
    "        agent.save_model()\n",
    "        print('One epoch of training finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac98b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c671b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
